{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "In this notebook I will implement the style transfer technique from [\"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\" (Johnson et al., ECCV 2016)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scipy.misc import imread, imresize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO Datasets (train split only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coco_train = tfds.load(name=\"coco2014\", split=tfds.Split.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the transform network.\n",
    "The architecture used in the paper [jcjohnson/fast-neural-style](https://github.com/jcjohnson/fast-neural-style/blob/master/doc/flags.md) is c9s1-32,d64,d128,R128,R128,R128,R128,R128,u64,u32,c9s1-3. All internal convolutional layers are followed by a ReLU and either batch normalization or instance normalization. <br>\n",
    "-  cXsY-Z: A convolutional layer with a kernel size of X, a stride of Y, and Z filters.\n",
    "-  dX: A downsampling convolutional layer with X filters, 3x3 kernels, and stride 2.\n",
    "-  RX: A residual block with two convolutional layers and X filters per layer.\n",
    "-  uX: An upsampling convolutional layer with X filters, 3x3 kernels, and stride 1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, regularizer=None):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2 = filters\n",
    "\n",
    "    self.conv2a = layers.Conv2D(filters1, kernel_size,\n",
    "                                padding='valid',\n",
    "                                kernel_regularizer=regularizer,\n",
    "                                bias_regularizer=regularizer)\n",
    "    self.bn2a = layers.BatchNormalization(beta_regularizer=regularizer,\n",
    "                                          gamma_regularizer=regularizer)\n",
    "    self.relu2a = layers.ReLU()\n",
    "\n",
    "    self.conv2b = layers.Conv2D(filters2, kernel_size,\n",
    "                                padding='valid',\n",
    "                                kernel_regularizer=regularizer,\n",
    "                                bias_regularizer=regularizer)\n",
    "    self.bn2b = layers.BatchNormalization(beta_regularizer=regularizer,\n",
    "                                          gamma_regularizer=regularizer)\n",
    "    self.crop2d = layers.Cropping2D(cropping=2)\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = self.relu2a(x)\n",
    "    \n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    \n",
    "    # center crop due to no padding in Conv2D\n",
    "    input_cropped = self.crop2d(input_tensor)\n",
    "    \n",
    "    return (x + input_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_pad(x):\n",
    "    paddings = tf.constant([[0,0],[40,40],[40,40],[0,0]])\n",
    "    return tf.pad(x, paddings, \"REFLECT\")\n",
    "\n",
    "l2 = None\n",
    "train_flag = True\n",
    "\n",
    "IMG_SIZE = 256\n",
    "input_tensor = layers.Input(shape=(None,None,3))\n",
    "input_padded = layers.Lambda(reflect_pad)(input_tensor)\n",
    "\n",
    "all_outputs = []\n",
    "# Add convolutional layer with a kernel size of 9x9, a stride of 1, and 32 filters\n",
    "conv1 = layers.Conv2D(filters=32, kernel_size=(9,9), strides=(1, 1),\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=l2,\n",
    "                      bias_regularizer=l2)(input_padded)\n",
    "conv1_bn = layers.BatchNormalization(beta_regularizer=l2,\n",
    "                                     gamma_regularizer=l2)(conv1, training=train_flag)\n",
    "conv1_relu = layers.ReLU()(conv1_bn)\n",
    "\n",
    "all_outputs.append(conv1)\n",
    "all_outputs.append(conv1_bn)\n",
    "all_outputs.append(conv1_relu)\n",
    "\n",
    "# Add downsampling convolutional layer with 64 filters, 3x3 kernels, and stride 2\n",
    "conv2 = layers.Conv2D(filters=64, kernel_size=(3,3), strides=(2,2),\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=l2,\n",
    "                      bias_regularizer=l2)(conv1_relu)\n",
    "conv2_bn = layers.BatchNormalization(beta_regularizer=l2,\n",
    "                                     gamma_regularizer=l2)(conv2, training=train_flag)\n",
    "conv2_relu = layers.ReLU()(conv2_bn)\n",
    "\n",
    "all_outputs.append(conv2)\n",
    "all_outputs.append(conv2_bn)\n",
    "all_outputs.append(conv2_relu)\n",
    "\n",
    "# Add downsampling convolutional layer with 128 filters, 3x3 kernels, and stride 2\n",
    "conv3 = layers.Conv2D(filters=128, kernel_size=(3,3), strides=(2,2),\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=l2,\n",
    "                      bias_regularizer=l2)(conv2_relu)\n",
    "conv3_bn = layers.BatchNormalization(beta_regularizer=l2,\n",
    "                                     gamma_regularizer=l2)(conv3, training=train_flag)\n",
    "conv3_relu = layers.ReLU()(conv3_bn)\n",
    "\n",
    "all_outputs.append(conv3)\n",
    "all_outputs.append(conv3_bn)\n",
    "all_outputs.append(conv3_relu)\n",
    "\n",
    "# Add residual block with two convolutional layers and 128 filters per layer\n",
    "x = ResnetIdentityBlock(kernel_size=3, filters=[128,128])(conv3_relu, training=train_flag)\n",
    "x = ResnetIdentityBlock(kernel_size=3, filters=[128,128])(x, training=train_flag)\n",
    "x = ResnetIdentityBlock(kernel_size=3, filters=[128,128])(x, training=train_flag)\n",
    "x = ResnetIdentityBlock(kernel_size=3, filters=[128,128])(x, training=train_flag)\n",
    "x = ResnetIdentityBlock(kernel_size=3, filters=[128,128])(x, training=train_flag)\n",
    "\n",
    "conv9 = layers.Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(2,2),\n",
    "                               padding='same',\n",
    "                               kernel_regularizer=l2,\n",
    "                               bias_regularizer=l2)(x)\n",
    "conv9_bn = layers.BatchNormalization(beta_regularizer=l2,\n",
    "                                     gamma_regularizer=l2)(conv9, training=train_flag)\n",
    "conv9_relu = layers.ReLU()(conv9_bn)\n",
    "\n",
    "all_outputs.append(conv9)\n",
    "all_outputs.append(conv9_bn)\n",
    "all_outputs.append(conv9_relu)\n",
    "\n",
    "conv10 = layers.Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2),\n",
    "                                padding='same',\n",
    "                                kernel_regularizer=l2,\n",
    "                                bias_regularizer=l2)(conv9_relu)\n",
    "conv10_bn = layers.BatchNormalization(beta_regularizer=l2,\n",
    "                                      gamma_regularizer=l2)(conv10, training=train_flag)\n",
    "conv10_relu = layers.ReLU()(conv10_bn)\n",
    "\n",
    "all_outputs.append(conv10)\n",
    "all_outputs.append(conv10_bn)\n",
    "all_outputs.append(conv10_relu)\n",
    "\n",
    "conv11 = layers.Conv2D(filters=3, kernel_size=(9,9), strides=(1, 1),\n",
    "                       padding='same',\n",
    "                       kernel_regularizer=l2,\n",
    "                       bias_regularizer=l2)(conv10_relu)\n",
    "conv11_tanh = layers.Lambda(tf.keras.backend.tanh)(conv11)\n",
    "\n",
    "all_outputs.append(conv11)\n",
    "all_outputs.append(conv11_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Setup\n",
    "Load the pretrained Keras VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    '''\n",
    "    model: an instance of Keras Application model\n",
    "    '''\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_layers = ['block1_conv2', 'block2_conv2',\n",
    "#                 'block3_conv3', 'block4_conv3']\n",
    "# thanks to https://github.com/OlavHN/fast-neural-style\n",
    "style_layers = ['block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1']\n",
    "_style_layer_weights = np.ones(len(style_layers)) * 5.0\n",
    "\n",
    "# content_layers = ['block3_conv3']\n",
    "content_layers = ['block3_conv2']\n",
    "_content_weights = np.ones(1)\n",
    "\n",
    "_tv_weight = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import os\n",
    "\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
    "freeze_model(vgg_model)\n",
    "\n",
    "content_feats = [vgg_model.get_layer(name).output for name in content_layers]\n",
    "style_feats = [vgg_model.get_layer(name).output for name in style_layers]\n",
    "vgg_model_x = tf.keras.Model(inputs=vgg_model.input,\n",
    "                             outputs=(content_feats+style_feats))\n",
    "\n",
    "# style_img_path = 'datasets/styles/starry_night_crop.png'\n",
    "style_img_path = 'datasets/styles/the_scream.jpg'\n",
    "\n",
    "img = image.load_img(style_img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "style_feats = vgg_model_x.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss\n",
    "\n",
    "We're going to compute the three components of our loss function now. The loss function is a weighted sum of three terms: content loss + style loss + total variation loss. You'll fill in the functions that compute these weighted terms below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n",
    "We can generate an image that reflects the content of one image and the style of another by incorporating both in our loss function. We want to penalize deviations from the content of the content image and deviations from the style of the style image. We can then use this hybrid loss function to perform gradient descent **not on the parameters** of the model, but instead **on the pixel values** of our original image.\n",
    "\n",
    "Let's first write the content loss function. Content loss measures how much the feature map of the generated image differs from the feature map of the source image. We only care about the content representation of selected layers of the network (say, $\\mathcal{L}$ is a set of K layers ). For a given layer $\\ell \\in \\mathcal{L}$, it has feature maps $A^\\ell \\in \\mathbb{R}^{1 \\times H_\\ell \\times W_\\ell \\times C_\\ell}$. $C_\\ell$ is the number of filters/channels in layer $\\ell$, $H_\\ell$ and $W_\\ell$ are the height and width. We will work with reshaped versions of these feature maps that combine all spatial positions into one dimension. Let $F^\\ell \\in \\mathbb{R}^{N_\\ell \\times M_\\ell}$ be the feature map for the current image and $P^\\ell \\in \\mathbb{R}^{N_\\ell \\times M_\\ell}$ be the feature map for the content source image where $M_\\ell=H_\\ell\\times W_\\ell$ is the number of elements in each feature map. Each row of $F^\\ell$ or $P^\\ell$ represents the vectorized activations of a particular filter, convolved over all positions of the image. Finally, let $w_c$ be the weight of the content loss term in the loss function.\n",
    "\n",
    "Then the content loss is given by:\n",
    "\n",
    "$L_c = w_c \\times \\sum_{\\ell} \\frac{1}{H_\\ell \\times W_\\ell \\times C_\\ell} \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_weights, content_current, content_original):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - content_weights: scalar constant we multiply the content_loss by.\n",
    "    - content_current: features of the current image, a list of Tensors.\n",
    "    - content_target: features of the content image, a list of Tensors, \n",
    "        with same shape as content_current.\n",
    "    \n",
    "    Returns:\n",
    "    - scalar content loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for i in np.arange(len(content_weights)):\n",
    "        loss += tf.reduce_mean((content_current[i] - content_original[i])**2) * content_weights[i]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style loss\n",
    "Now we can tackle the style loss. For a given layer $\\ell$, the style loss is defined as follows:\n",
    "\n",
    "First, compute the Gram matrix G which represents the correlations between the responses of each filter, where F is as above. The Gram matrix is an approximation to the covariance matrix -- we want the activation statistics of our generated image to match the activation statistics of our style image, and matching the (approximate) covariance is one way to do that. There are a variety of ways you could do this, but the Gram matrix is nice because it's easy to compute and in practice shows good results.\n",
    "\n",
    "Given a feature map $F^\\ell$ of shape $(1, M_\\ell, C_\\ell)$, the Gram matrix has shape $(C_\\ell, C_\\ell)$ and its elements are given by:\n",
    "\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "\n",
    "Assuming $G^\\ell$ is the Gram matrix from the feature map of the current image, $A^\\ell$ is the Gram Matrix from the feature map of the source style image, and $w_\\ell$ a scalar weight term, then the style loss for the layer $\\ell$ is simply the weighted Euclidean distance between the two Gram matrices:\n",
    "\n",
    "$$L_s^\\ell = w_\\ell \\times \\frac{1}{C_\\ell \\times C_\\ell} \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "In practice we usually compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $\\ell$; then the total style loss is the sum of style losses at each layer:\n",
    "\n",
    "$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n",
    "\n",
    "Begin by implementing the Gram matrix computation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(features, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix from features.\n",
    "    \n",
    "    Inputs:\n",
    "    - features: Tensor of shape (N, H, W, C) giving features for\n",
    "      N images.\n",
    "    - normalize: optional, whether to normalize the Gram matrix\n",
    "        If True, divide the Gram matrix by the number of neurons (H * W * C)\n",
    "        H = tf.getshape(features)[1]\n",
    "    Returns:\n",
    "    - gram: Tensor of shape (N, C, C) giving the (optionally normalized)\n",
    "      Gram matrices for the input images.\n",
    "    \"\"\"\n",
    "    N, H, W, C = tf.shape(features)\n",
    "    if normalize is True:\n",
    "        features /= tf.sqrt(tf.cast(H*W*C, features.dtype))\n",
    "    features = tf.reshape(features, (N,-1,C))\n",
    "    features_T = tf.transpose(features, (0,2,1))\n",
    "    grams = tf.matmul(features_T, features)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the style loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def style_loss(feats, style_targets, style_weights):\n",
    "    \"\"\"\n",
    "    Computes the style loss at a set of layers.\n",
    "    \n",
    "    Inputs:\n",
    "    - feats: list of the features at every layer of the current image.\n",
    "    - style_targets: List of the same length as feats, where style_targets[i] is\n",
    "      a Tensor giving the Gram matrix the source style image computed at\n",
    "      layer style_layers[i].\n",
    "    - style_weights: List of the same length as style_targets, where style_weights[i]\n",
    "      is a scalar giving the weight for the style loss at layer style_layers[i].\n",
    "      \n",
    "    Returns:\n",
    "    - style_loss: A Tensor contataining the scalar style loss.\n",
    "    \"\"\"\n",
    "    style_loss = tf.constant(0.0)\n",
    "    for i in np.arange(len(feats)):\n",
    "        layer_var = gram_matrix(feats[i])\n",
    "        with tf.contrib.summary.always_record_summaries():\n",
    "            tf.contrib.summary.histogram('sample_grams' + str(i), layer_var)\n",
    "        loss_i = tf.reduce_mean((layer_var - style_targets[i])**2) * style_weights[i]\n",
    "        style_loss = tf.add(style_loss, loss_i)\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total-variation regularization\n",
    "It turns out that it's helpful to also encourage smoothness in the image. We can do this by adding another term to our loss that penalizes wiggles or \"total variation\" in the pixel values. \n",
    "\n",
    "You can compute the \"total variation\" as the sum of the squares of differences in the pixel values for all pairs of pixels that are next to each other (horizontally or vertically). Here we sum the total-variation regualarization for each of the 3 input channels (RGB), and weight the total summed loss by the total variation weight, $w_t$:\n",
    "\n",
    "$L_{tv} = w_t \\times \\frac{1}{C \\times (H-1) \\times (W-1)} \\times \\sum_{c=1}^3\\sum_{i=1}^{H-1} \\sum_{j=1}^{W-1} \\left( (x_{i,j+1, c} - x_{i,j,c})^2 + (x_{i+1, j,c} - x_{i,j,c})^2  \\right)$\n",
    "\n",
    "In the next cell, fill in the definition for the TV loss term. To receive full credit, your implementation should not have any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(img, tv_weight):\n",
    "    \"\"\"\n",
    "    Compute total variation loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - img: Tensor of shape (N, H, W, 3) holding input images\n",
    "    - tv_weight: Scalar giving the weight w_t to use for the TV loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Tensor holding a scalar giving the total variation loss\n",
    "      for img weighted by tv_weight.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(img)\n",
    "    A = tf.slice(img, [0, 0, 0, 0], shape - [0, 1, 1, 0])\n",
    "    B = tf.slice(img, [0, 1, 0, 0], shape - [0, 1, 1, 0])\n",
    "    C = tf.slice(img, [0, 0, 1, 0], shape - [0, 1, 1, 0])\n",
    "    loss = tv_weight * tf.reduce_sum((A - B)**2 + (A - C)**2) / tf.cast(shape[0], tf.float32)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image_utils import preprocess_image\n",
    "\n",
    "VGG_MEAN_BGR = np.array([103.939, 116.779, 123.68], dtype=np.float32)\n",
    "\n",
    "def total_loss(model, orig_imgs, style_grams, content_weights, style_layer_weights, tv_weight):\n",
    "    y_pred = model(orig_imgs)\n",
    "    for i in np.arange(len(y_pred)):\n",
    "        tf.check_numerics(y_pred[i], 'output' + str(i))\n",
    "        \n",
    "    y_pred_restored = (y_pred[-1] + 1) / 2 * 256\n",
    "    y_pred_norm = y_pred_restored - VGG_MEAN_BGR[None,None,None]\n",
    "    \n",
    "    orig_feats = vgg_model_x(orig_imgs)\n",
    "    y_pred_feats = vgg_model_x(y_pred_norm)\n",
    "    \n",
    "    IDX = len(content_weights)\n",
    "    c_loss = content_loss(content_weights, y_pred_feats[:IDX], orig_feats[:IDX]) \n",
    "    # print weights to verify that vgg model is freezed\n",
    "    s_loss = style_loss(y_pred_feats[IDX:], style_grams, style_layer_weights)\n",
    "    \n",
    "    t_loss = tv_loss(y_pred_norm, tv_weight)\n",
    "\n",
    "    with tf.contrib.summary.record_summaries_every_n_global_steps(20):\n",
    "#     with tf.contrib.summary.always_record_summaries():\n",
    "        tf.contrib.summary.scalar('content_loss', c_loss)\n",
    "        tf.contrib.summary.scalar('style_loss', s_loss)\n",
    "        tf.contrib.summary.scalar('tv_loss', t_loss)\n",
    "        for i in np.arange(len(y_pred)):\n",
    "            tf.contrib.summary.histogram('output' + str(i), y_pred[i])\n",
    "        for i in np.arange(len(y_pred_feats)):\n",
    "            tf.contrib.summary.histogram('y_pred_feats' + str(i), y_pred_feats[i])\n",
    "        for i in np.arange(len(orig_feats)):\n",
    "            tf.contrib.summary.histogram('orig_feats' + str(i), orig_feats[i])\n",
    "    return c_loss + s_loss + t_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_img_path = 'datasets/examples/tubingen.jpg'\n",
    "img = image.load_img(test_img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "orig_img = image.img_to_array(img)\n",
    "orig_img = np.expand_dims(orig_img, axis=0)\n",
    "orig_img_norm = preprocess_input(orig_img)\n",
    "\n",
    "IDX = len(content_layers)\n",
    "style_grams = [gram_matrix(layer) for layer in style_feats[IDX:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, orig_imgs, style_grams, content_weights, style_layer_weights, tv_weight):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = total_loss(model, orig_imgs, style_grams, content_weights, style_layer_weights, tv_weight)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.image_utils import preprocess_image, deprocess_image\n",
    "import gc\n",
    "\n",
    "# del model\n",
    "# tf.keras.backend.clear_session()\n",
    "# gc.collect()\n",
    "\n",
    "def _pre_process(x):\n",
    "    image_resized = tf.image.resize_images(x['image'], [IMG_SIZE, IMG_SIZE])\n",
    "    image_centered = preprocess_image(image_resized[None])\n",
    "    return image_centered[0]\n",
    "\n",
    "# lr_space = np.power(10.0, np.arange(-4,2)) * 1.e-3\n",
    "# weight_space = np.power(10.0, np.arange(-4,1))\n",
    "# lr_space = [1e-7, 1e-7, 1e-6]\n",
    "# weight_scale = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "logdir = './tb/scream/scream_lre-4_7.5_1e2_2e2'\n",
    "writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "writer.set_as_default()\n",
    "\n",
    "dataset = coco_train.repeat(2).map(_pre_process).batch(4).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "global_step.assign(0)\n",
    "\n",
    "lr = 1e-4\n",
    "content_weights = np.ones(1) * 7.5\n",
    "style_layer_weights = np.ones(4) * 1e2\n",
    "# style_layer_weights[0] = 5e2\n",
    "# style_layer_weights[1] = 5e2\n",
    "# style_layer_weights[2] = 5e2\n",
    "# style_layer_weights[3] = 5e2\n",
    "\n",
    "tv_weight = _tv_weight * 2e2\n",
    "\n",
    "print('lr: {}; content_weights: {}; style_weights: {}; tv_weight: {}'.\n",
    "      format(lr, content_weights, style_layer_weights, tv_weight))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=all_outputs)\n",
    "# model.load_weights(logdir+'/training_checkpoint')\n",
    "# model.save_weights('./my_checkpoint')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "with tf.contrib.summary.always_record_summaries():\n",
    "    for k in np.arange(len(style_grams)):\n",
    "        tf.contrib.summary.histogram('style_gram' + str(k), style_grams[k])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        batch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        x = iterator.get_next()\n",
    "        loss_value, grads = grad(model, x, style_grams, content_weights,\n",
    "                                 style_layer_weights, tv_weight)\n",
    "        for i in np.arange(len(grads)):\n",
    "            tf.check_numerics(grads[i], 'check_numerics' + str(i))\n",
    "#         for i in np.arange(len(grads)):\n",
    "#             shape = np.shape(grads[i])\n",
    "#             weight_scale = np.linalg.norm(tf.reshape(model.trainable_variables[i], [-1]))\n",
    "#             update_scale = np.linalg.norm(tf.reshape(grads[i] * lr, [-1]))\n",
    "#             update_ratio = update_scale / weight_scale\n",
    "#             temp = []\n",
    "#             if update_ratio > 0.1 or update_ratio < 1e-5 : \n",
    "#                 temp.append(i)\n",
    "#                 temp.append(shape)\n",
    "#                 temp.append(update_ratio)\n",
    "#                 temp.append(np.mean(np.abs(update_scale)))\n",
    "#                 temp.append(np.mean(np.abs(weight_scale)))\n",
    "#             if len(temp) > 0 :\n",
    "#                 print(*temp)\n",
    "#                 temp = []\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
    "                                  global_step)\n",
    "        batch_loss_avg(loss_value)\n",
    "        # Track progress\n",
    "        if global_step.numpy() % 100 == 0:\n",
    "            print(\"Step {:03d}: Loss: {:.3f}\".format(global_step.numpy(), batch_loss_avg.result()))\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(20):\n",
    "#         with tf.contrib.summary.always_record_summaries():\n",
    "            tf.contrib.summary.scalar('total_loss', batch_loss_avg.result())\n",
    "            batch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "            for i in np.arange(len(grads)):\n",
    "                tf.contrib.summary.histogram('grads'+str(i), grads[i])\n",
    "                tf.contrib.summary.histogram('weights'+str(i), model.trainable_variables[i])\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(200):\n",
    "            sample_outputs = model.predict(orig_img_norm)\n",
    "            sample_output_pp = ((sample_outputs[-1] + 1) / 2 * 256).astype(np.uint8)\n",
    "            tf.contrib.summary.image('test', sample_output_pp[...,::-1], max_images=1, step=global_step.numpy())\n",
    "        if global_step.numpy() % 1000 == 0:\n",
    "            model.save_weights(logdir+'/training_checkpoint')\n",
    "#         if global_step.numpy() % 3000 == 0:\n",
    "#             break\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        model.save_weights('./training_checkpoint')\n",
    "        break\n",
    "#         pred_img = model.predict(orig_img_norm)\n",
    "#         pred_img_restore = ((pred_img[-1] + 1) / 2 * 256).astype(np.uint8)https://127.0.0.1:7000/notebooks/my-neural-style/StyleTransfer.ipynb#\n",
    "#         plt.imshow(pred_img_restore[0][..., ::-1])\n",
    "# del model\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=all_outputs)\n",
    "logdir = './tb/scream/scream_lre-4_7.5_1e2_2e2'\n",
    "model.load_weights(logdir+'/training_checkpoint')\n",
    "\n",
    "test_img_path = 'datasets/examples/tubingen.jpg'\n",
    "img = image.load_img(test_img_path, target_size=(512, 512))\n",
    "orig_img = image.img_to_array(img)\n",
    "orig_img_4d = np.expand_dims(orig_img, axis=0)\n",
    "orig_img_norm = preprocess_input(orig_img_4d)\n",
    "pred_img = model.predict(orig_img_norm)\n",
    "pred_img_restore = ((pred_img[-1] + 1) / 2 * 256).astype(np.uint8)\n",
    "plt.imshow(pred_img_restore[0][..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(inputs=input_tensor, outputs=all_outputs)\n",
    "# model.load_weights(logdir+'./training_checkpoint')\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "test_img_path = 'datasets/examples/hoovertowernight.jpg'\n",
    "img = image.load_img(test_img_path, target_size=(512, 512))\n",
    "orig_img = image.img_to_array(img)\n",
    "orig_img_4d = np.expand_dims(orig_img, axis=0)\n",
    "orig_img_norm = preprocess_input(orig_img_4d)\n",
    "pred_img = model.predict(orig_img_norm)\n",
    "pred_img_restore = ((pred_img[-1] + 1) / 2 * 256).astype(np.uint8)\n",
    "plt.imshow(pred_img_restore[0][..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put it all together and make some beautiful images! The `style_transfer` function below combines all the losses you coded up above and optimizes for an image that minimizes the total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some pretty pictures!\n",
    "\n",
    "Try out `style_transfer` on the three different parameter sets below. Make sure to run all three cells. Feel free to add your own, but make sure to include the results of style transfer on the third parameter set (starry night) in your submitted notebook.\n",
    "\n",
    "* The `content_image` is the filename of content image.\n",
    "* The `style_image` is the filename of style image.\n",
    "* The `image_size` is the size of smallest image dimension of the content image (used for content loss and generated image).\n",
    "* The `style_size` is the size of smallest style image dimension.\n",
    "* The `content_layer` specifies which layer to use for content loss.\n",
    "* The `content_weight` gives weighting on content loss in the overall loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).\n",
    "* `style_layers` specifies a list of which layers to use for style loss. \n",
    "* `style_weights` specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.\n",
    "* `tv_weight` specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content. \n",
    "\n",
    "Below the next three cells of code (in which you shouldn't change the hyperparameters), feel free to copy and paste the parameters to play around them and see how the resulting image changes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
